# Incident Response Plan

> **Comprehensive guide for responding to production incidents in My-SaaS-Chat**
> **Last Updated:** 2025-11-15
> **Version:** 2.0

## Purpose

This document provides structured procedures for identifying, responding to, and resolving production incidents.

**Goals:**
- Minimize downtime and user impact
- Restore service as quickly as possible
- Preserve data integrity
- Learn from incidents to prevent recurrence

---

## Table of Contents

- [Incident Classification](#incident-classification)
- [Incident Response Team](#incident-response-team)
- [Incident Response Workflow](#incident-response-workflow)
- [Common Incident Scenarios](#common-incident-scenarios)
- [Communication Templates](#communication-templates)
- [Post-Incident Process](#post-incident-process)
- [On-Call Procedures](#on-call-procedures)

---

## Incident Classification

### Severity Levels

#### SEV-1: Critical

**Definition:** Complete service outage or critical functionality unavailable

**Criteria:**
- All users unable to access service (login broken)
- Payment processing completely down
- Data corruption or loss occurring
- Security breach in progress
- Error rate > 50%

**Response Time:**
- Detection to acknowledgment: < 5 minutes
- Acknowledgment to mitigation: < 15 minutes
- Target resolution: < 1 hour

**Escalation:** Immediate (all hands on deck)

**Examples:**
- Database completely down
- All authentication failing
- Major security vulnerability actively exploited
- Service completely unreachable

---

#### SEV-2: High

**Definition:** Major functionality impacted, significant user impact

**Criteria:**
- Critical feature unavailable (chat, billing)
- Error rate 10-50%
- Performance degraded >5x normal
- Affecting >50% of users
- Data integrity concerns

**Response Time:**
- Detection to acknowledgment: < 15 minutes
- Acknowledgment to mitigation: < 30 minutes
- Target resolution: < 4 hours

**Escalation:** On-call + DevOps lead

**Examples:**
- Chat service down (auth still works)
- Billing checkout failing
- Severe performance degradation
- Partial database failure

---

#### SEV-3: Medium

**Definition:** Moderate impact to functionality or performance

**Criteria:**
- Non-critical feature unavailable
- Error rate 5-10%
- Performance degraded 2-5x normal
- Affecting 10-50% of users
- Workaround available

**Response Time:**
- Detection to acknowledgment: < 30 minutes
- Acknowledgment to mitigation: < 2 hours
- Target resolution: < 8 hours

**Escalation:** On-call engineer

**Examples:**
- Analytics service down
- Email delivery delayed
- Moderate performance issues
- Non-critical API endpoints failing

---

#### SEV-4: Low

**Definition:** Minor issues with minimal user impact

**Criteria:**
- Minor bug in UI
- Error rate 1-5%
- Performance slightly degraded (<2x)
- Affecting <10% of users
- No business impact

**Response Time:**
- Detection to acknowledgment: < 1 hour
- Acknowledgment to mitigation: < 4 hours
- Target resolution: Next business day

**Escalation:** Not required (log and fix)

**Examples:**
- UI cosmetic issues
- Minor logging errors
- Occasional timeout (not pattern)
- Non-critical feature glitch

---

## Incident Response Team

### Roles and Responsibilities

#### Incident Commander (IC)

**Primary:** On-call engineer
**Backup:** DevOps lead

**Responsibilities:**
- Overall incident coordination
- Make key decisions (rollback, escalation)
- Declare incident severity
- Coordinate communication
- Lead post-mortem

**Authority:**
- Execute rollback without approval
- Engage additional resources
- Make deployment decisions

---

#### Technical Lead

**Primary:** Senior backend engineer
**Backup:** Tech lead

**Responsibilities:**
- Deep-dive technical investigation
- Implement fixes
- Review code changes
- Validate solutions

---

#### Communications Lead

**Primary:** Product manager
**Backup:** Engineering manager

**Responsibilities:**
- Update status page
- Customer communications
- Internal stakeholder updates
- Support team liaison

---

#### Support:**

**Primary:** On-call support engineer
**Backup:** Support team lead

**Responsibilities:**
- Monitor support tickets
- Escalate critical user reports
- Communicate with affected users
- Track user impact

---

## Incident Response Workflow

### Phase 1: Detection & Triage (0-5 minutes)

#### Step 1: Incident Detected

**Automated:**
- [ ] Alert received (PagerDuty, Slack, email)
- [ ] Monitoring system detected anomaly
- [ ] Health check failed

**Manual:**
- [ ] User report received
- [ ] Team member noticed issue
- [ ] Support ticket escalated

#### Step 2: Acknowledge

- [ ] Acknowledge alert in PagerDuty (stops escalation)
- [ ] Join incident Slack channel: #incident-YYYYMMDD-XX
- [ ] Post initial acknowledgment:
  ```
  "Incident acknowledged. Investigating <brief description>.
   IC: @yourname"
  ```

#### Step 3: Triage

- [ ] **Verify the incident:**
  ```bash
  # Check service health
  curl https://api.yourdomain.com/health

  # Check error rate
  curl https://api.yourdomain.com/api/analytics/errors/rate

  # Check logs
  docker compose logs --tail=100 -f
  ```

- [ ] **Assess severity:**
  - How many users affected?
  - What functionality is broken?
  - Is data at risk?
  - Can users work around it?

- [ ] **Declare severity:** SEV-1, SEV-2, SEV-3, or SEV-4

- [ ] **Update incident channel:**
  ```
  "Incident classified as SEV-X: <description>
   Estimated impact: X users, Y functionality
   Next step: <action>"
  ```

---

### Phase 2: Investigation (5-30 minutes)

#### Step 1: Gather Data

- [ ] **Check monitoring dashboards:**
  - Grafana: https://api.yourdomain.com:3100
  - Prometheus: https://api.yourdomain.com:9090
  - Jaeger: https://api.yourdomain.com:16686

- [ ] **Review logs:**
  ```bash
  # All services
  docker compose logs --tail=500 -f | grep -i error

  # Specific service
  docker compose logs -f <service-name>

  # Time range
  docker compose logs --since="15m" <service-name>
  ```

- [ ] **Check recent changes:**
  ```bash
  # Recent deployments
  git log -10 --oneline

  # Recent commits
  git log --since="2 hours ago"

  # Running containers
  docker compose ps
  ```

- [ ] **Identify error patterns:**
  - Common error messages
  - Affected endpoints
  - Correlated metrics (CPU spike, memory leak)

#### Step 2: Form Hypothesis

- [ ] What changed recently?
- [ ] What are the symptoms?
- [ ] What is the likely root cause?

**Example:**
```
Hypothesis: Chat service failing due to database connection pool exhaustion
Evidence: Logs show "connection pool timeout" errors
Timeline: Started 15 minutes ago after chat traffic spike
```

#### Step 3: Document Findings

- [ ] Update incident channel with hypothesis
- [ ] Share relevant logs/screenshots
- [ ] Timeline of events

---

### Phase 3: Mitigation (Concurrent with Investigation)

#### Option A: Quick Fix

**If fix is obvious and can be applied < 5 minutes:**

- [ ] Implement fix
- [ ] Deploy to production
- [ ] Verify fix resolves issue
- [ ] Monitor for 15 minutes

**Examples:**
- Restart crashed service
- Fix configuration typo
- Scale up resources
- Clear cache

#### Option B: Rollback

**If fix will take > 15 minutes OR data integrity at risk:**

- [ ] Execute rollback procedure (see ROLLBACK_RUNBOOK.md)
- [ ] Follow Procedure A (code only) or B (code + database)
- [ ] Verify services restored
- [ ] Investigate root cause after stability restored

#### Option C: Workaround

**If fix and rollback not immediately viable:**

- [ ] Implement temporary workaround
- [ ] Update status page with workaround
- [ ] Continue working on permanent fix

**Examples:**
- Redirect traffic to backup service
- Disable failing feature
- Rate limit affected endpoint
- Manual processing of failed requests

---

### Phase 4: Resolution (30 min - 4 hours)

#### Step 1: Implement Solution

- [ ] Code fix developed
- [ ] Tested in staging environment
- [ ] Peer review completed
- [ ] Deployment plan approved

#### Step 2: Deploy

- [ ] Deploy fix to production
- [ ] Monitor deployment closely
- [ ] Verify fix resolves issue
- [ ] No new errors introduced

#### Step 3: Verify

- [ ] All health checks passing
- [ ] Error rate returned to normal (<0.1%)
- [ ] Response times back to baseline
- [ ] Critical user flows working
- [ ] No reports of ongoing issues

#### Step 4: Monitor

- [ ] Watch metrics for 30 minutes
- [ ] Verify stability
- [ ] Check for secondary issues
- [ ] Review user feedback

---

### Phase 5: Communication

#### During Incident

- [ ] **Status Page Updates:**
  - Initial: "Investigating issue with <service>"
  - Mitigation: "Fix applied, monitoring for stability"
  - Resolution: "Issue resolved. Services operating normally."

- [ ] **Slack Updates (every 15 min for SEV-1/2):**
  ```
  "Update: <current status>
   Actions taken: <summary>
   Next steps: <plan>
   ETA: <estimate>"
  ```

- [ ] **Support Team:**
  - Provide talking points
  - Share workaround (if available)
  - Update on resolution progress

#### After Resolution

- [ ] **Final Status Page Update:**
  ```
  "Resolved: <brief description>
   Duration: <time>
   Impact: <summary>
   Root cause: <one sentence>
   Prevention: <actions taken>"
  ```

- [ ] **Internal Notification:**
  ```
  "Incident resolved: SEV-X
   Duration: <time>
   Root cause: <summary>
   Post-mortem: <date/time>"
  ```

---

### Phase 6: Post-Incident

- [ ] Schedule post-mortem (24-48 hours after resolution)
- [ ] Prepare post-mortem document
- [ ] Create action items
- [ ] Update incident log
- [ ] Share learnings with team

**See [Post-Incident Process](#post-incident-process) for details.**

---

## Common Incident Scenarios

### Scenario 1: Service Outage

**Symptoms:**
- Health checks failing
- Users unable to access service
- High error rate (>50%)

**Investigation:**
```bash
# Check service status
docker compose ps

# Check logs
docker compose logs --tail=100 <service>

# Check resource usage
docker stats
```

**Common Causes:**
- Service crashed (OOM, unhandled exception)
- Database connection failed
- Resource exhaustion
- Dependency unavailable

**Mitigation:**
```bash
# Quick restart
docker compose restart <service>

# Or rollback if restart doesn't work
./scripts/quick-rollback.sh <previous-version>
```

**Prevention:**
- Add better error handling
- Implement circuit breakers
- Increase resource limits
- Add health checks to dependencies

---

### Scenario 2: Performance Degradation

**Symptoms:**
- Slow response times (>5x normal)
- Timeout errors increasing
- Users reporting slowness

**Investigation:**
```bash
# Check response times
curl -w "@curl-format.txt" https://api.yourdomain.com/api/chats

# Check database query performance
docker compose exec postgres psql -U postgres -d saas_db -c "
  SELECT query, calls, total_time, mean_time
  FROM pg_stat_statements
  ORDER BY mean_time DESC
  LIMIT 10;
"

# Check slow queries
docker compose logs postgres | grep "duration"
```

**Common Causes:**
- Database query not optimized (missing index, N+1)
- High traffic spike
- Memory leak
- External API slow

**Mitigation:**
- Scale up resources temporarily
- Add caching for hot queries
- Rate limit if traffic spike
- Optimize slow queries

**Prevention:**
- Add query performance monitoring
- Set up auto-scaling
- Implement query timeout limits
- Regular performance testing

---

### Scenario 3: Database Connection Issues

**Symptoms:**
- "Connection pool exhausted" errors
- "Too many connections" errors
- Services can't connect to database

**Investigation:**
```bash
# Check active connections
docker compose exec postgres psql -U postgres -c "
  SELECT COUNT(*) FROM pg_stat_activity;
"

# Check connection limit
docker compose exec postgres psql -U postgres -c "
  SHOW max_connections;
"

# Check by service
docker compose exec postgres psql -U postgres -c "
  SELECT application_name, COUNT(*)
  FROM pg_stat_activity
  GROUP BY application_name;
"
```

**Common Causes:**
- Connection leak (not closing connections)
- Traffic spike exceeding pool size
- Long-running queries blocking connections
- Service restart without closing connections

**Mitigation:**
```bash
# Terminate idle connections
docker compose exec postgres psql -U postgres -c "
  SELECT pg_terminate_backend(pid)
  FROM pg_stat_activity
  WHERE state = 'idle'
  AND state_change < NOW() - INTERVAL '5 minutes';
"

# Increase max_connections (temporary)
docker compose exec postgres psql -U postgres -c "
  ALTER SYSTEM SET max_connections = 200;
"

# Restart PostgreSQL
docker compose restart postgres
```

**Prevention:**
- Review connection pool configuration
- Add connection leak detection
- Implement connection timeout
- Monitor connection usage

---

### Scenario 4: High Error Rate

**Symptoms:**
- Error rate suddenly increases (>5%)
- Specific endpoint failing
- Errors in logs

**Investigation:**
```bash
# Check error distribution
curl https://api.yourdomain.com/api/analytics/errors/distribution

# Check recent errors
docker compose logs --tail=500 | grep -i "error" | tail -20

# Check Sentry
# https://sentry.io/organizations/.../issues/
```

**Common Causes:**
- Recent deployment introduced bug
- External service failing (Stripe, OpenAI)
- Invalid input from users
- Rate limit hit on external API

**Mitigation:**
- Rollback if recent deployment
- Implement fallback for external service
- Add input validation
- Increase rate limits or add retry logic

**Prevention:**
- Better test coverage
- Circuit breakers for external services
- Input validation middleware
- Canary deployments

---

### Scenario 5: Memory Leak

**Symptoms:**
- Memory usage increasing over time
- OOM (Out of Memory) kills
- Service crashes after running for hours

**Investigation:**
```bash
# Check memory usage trends
docker stats --no-stream

# Check heap usage (Node.js)
docker compose exec <service> curl http://localhost:<port>/metrics \
  | grep heap

# Check for memory leaks in logs
docker compose logs <service> | grep -i "heap\|memory"
```

**Common Causes:**
- Event listeners not removed
- Cache growing unbounded
- Circular references
- Large objects not released

**Mitigation:**
```bash
# Restart service (temporary)
docker compose restart <service>

# Reduce cache size
# Update MAX_CACHE_SIZE in .env

# Increase memory limit (temporary)
# Edit docker-compose.prod.yml memory limits
```

**Prevention:**
- Add memory profiling
- Set cache size limits
- Regular service restarts
- Code review for memory leaks

---

### Scenario 6: External Service Failure

**Symptoms:**
- OpenAI API calls failing
- Stripe checkout not working
- Email delivery failing

**Investigation:**
```bash
# Check external service status pages
# OpenAI: https://status.openai.com
# Stripe: https://status.stripe.com
# SendGrid: https://status.sendgrid.com

# Check error logs
docker compose logs <service> | grep -i "openai\|stripe\|sendgrid"

# Check network connectivity
docker compose exec <service> curl https://api.openai.com
```

**Common Causes:**
- External service outage
- API key expired/invalid
- Rate limit exceeded
- Network issues

**Mitigation:**
- Implement fallback provider (e.g., Cloudflare for OpenAI)
- Graceful degradation (disable feature temporarily)
- Queue failed requests for retry
- Update status page

**Prevention:**
- Multi-provider support
- Circuit breakers
- Retry with exponential backoff
- Monitor external service status

---

### Scenario 7: Security Incident

**Symptoms:**
- Unusual traffic patterns
- Failed authentication spike
- Unauthorized access detected
- Suspicious database queries

**Investigation:**
```bash
# Check access logs
docker compose logs nginx | grep -i "401\|403"

# Check failed login attempts
docker compose exec postgres psql -U postgres -d saas_db -c "
  SELECT * FROM audit_log
  WHERE action = 'LOGIN_FAILED'
  AND created_at > NOW() - INTERVAL '1 hour'
  ORDER BY created_at DESC;
"

# Check unusual activity
docker compose logs | grep -i "unauthorized\|forbidden\|suspicious"
```

**Common Causes:**
- Brute force attack
- SQL injection attempt
- XSS attack
- Leaked credentials
- Insider threat

**Mitigation:**
```bash
# Block malicious IPs
# Add to nginx blocklist

# Rotate compromised credentials
# Update .env and restart services

# Enable additional security measures
# Rate limiting, 2FA, etc.

# If data breach suspected
# Execute emergency shutdown (see ROLLBACK_RUNBOOK.md Procedure D)
```

**Prevention:**
- Regular security audits
- Input validation
- Rate limiting
- Security headers
- Regular credential rotation
- Security awareness training

---

## Communication Templates

### Status Page Updates

#### Investigating

```
We are investigating an issue affecting <service/feature>.
Users may experience <impact>.
We will provide updates as we learn more.

Updated: <timestamp>
```

#### Identified

```
We have identified the issue affecting <service/feature>.
Root cause: <brief description>.
Our team is working on a fix.
Expected resolution: <ETA>.

Updated: <timestamp>
```

#### Monitoring

```
A fix has been applied for the issue affecting <service/feature>.
We are monitoring the situation to ensure stability.

Updated: <timestamp>
```

#### Resolved

```
The issue affecting <service/feature> has been resolved.
Duration: <X minutes/hours>
Impact: <brief summary>
Root cause: <one sentence>

We apologize for any inconvenience.

Updated: <timestamp>
```

---

### Internal Slack Updates

#### Initial

```
ðŸš¨ INCIDENT: SEV-<X>
Description: <brief description>
Impact: <users/services affected>
IC: @<yourname>
Investigating: <hypothesis>

Incident channel: #incident-<YYYYMMDD>-<XX>
```

#### Progress Update

```
ðŸ“Š UPDATE (<time> elapsed)
Status: <investigating/mitigating/resolved>
Actions taken:
  - <action 1>
  - <action 2>
Next steps:
  - <next action>
ETA: <estimate>
```

#### Resolution

```
âœ… RESOLVED
Duration: <X minutes>
Root cause: <brief explanation>
Fix: <what was done>
Impact: <summary>

Post-mortem: Scheduled for <date/time>
Thanks to: <team members who helped>
```

---

### Support Team Communication

#### Initial

```
ALERT: Service Issue (<severity>)

What's happening:
- <brief description>
- Affects: <which users/features>

What to tell customers:
"We're aware of an issue affecting <feature>.
Our team is working on a fix.
Updates: <status page URL>"

DO NOT:
- Provide technical details
- Make promises about ETA (unless confirmed)

Questions? Ask in #incident-<YYYYMMDD>-<XX>
```

#### Resolution

```
RESOLVED: Service Issue

The issue has been resolved.
Duration: <X minutes>

What to tell customers:
"The issue affecting <feature> has been resolved.
We apologize for any inconvenience."

If customers report ongoing issues:
Escalate to #incident-<YYYYMMDD>-<XX>
```

---

## Post-Incident Process

### Post-Mortem Document Template

```markdown
# Incident Post-Mortem: <Date>

## Metadata
- **Incident ID:** INC-<YYYYMMDD>-<XX>
- **Severity:** SEV-<X>
- **Date:** <YYYY-MM-DD>
- **Duration:** <X hours Y minutes>
- **Incident Commander:** <Name>
- **Participants:** <Names>

## Summary
<2-3 sentence overview of what happened>

## Impact
- **Users Affected:** <number> (<percentage>%)
- **Services Affected:** <list>
- **Downtime:** <total minutes>
- **Revenue Impact:** $<estimated amount>
- **Support Tickets:** <number>

## Timeline (All times in UTC)

| Time | Event |
|------|-------|
| HH:MM | Incident detected (alert fired) |
| HH:MM | Incident acknowledged |
| HH:MM | Investigation started |
| HH:MM | Root cause identified |
| HH:MM | Mitigation applied |
| HH:MM | Services restored |
| HH:MM | Incident resolved |

## Root Cause

### What Happened
<Detailed technical explanation>

### Why It Happened
<Underlying cause, not just symptoms>

### Why Wasn't It Caught Earlier
- Testing: <what was missed in testing>
- Monitoring: <what alerts didn't fire>
- Code Review: <what was missed in review>

## What Went Well
- <Thing 1>
- <Thing 2>

## What Went Wrong
- <Thing 1>
- <Thing 2>

## Action Items

### Immediate (This Week)
- [ ] <Action 1> - Owner: <name> - ETA: <date>
- [ ] <Action 2> - Owner: <name> - ETA: <date>

### Short-term (This Month)
- [ ] <Action 1> - Owner: <name> - ETA: <date>
- [ ] <Action 2> - Owner: <name> - ETA: <date>

### Long-term (This Quarter)
- [ ] <Action 1> - Owner: <name> - ETA: <date>
- [ ] <Action 2> - Owner: <name> - ETA: <date>

## Lessons Learned
- <Lesson 1>
- <Lesson 2>

## Appendix
- Logs: <link>
- Metrics: <link>
- Slack thread: <link>
```

### Post-Mortem Meeting Agenda

1. **Review Timeline (10 min)**
   - Walk through incident timeline
   - Clarify any questions

2. **Discuss Root Cause (15 min)**
   - Technical deep-dive
   - Contributing factors
   - Why it wasn't caught

3. **What Went Well/Wrong (10 min)**
   - Detection and response
   - Communication
   - Tools and processes

4. **Action Items (15 min)**
   - Preventative measures
   - Process improvements
   - Assign owners and deadlines

5. **Lessons Learned (10 min)**
   - What would we do differently?
   - How can we improve?

**Total:** 1 hour

**Attendees:** IC, Technical Lead, DevOps, Product Manager, affected stakeholders

---

## On-Call Procedures

### Before Your Shift

- [ ] Review recent incidents and ongoing issues
- [ ] Test PagerDuty notifications (SMS, phone, email)
- [ ] Verify access to all necessary systems
- [ ] Review this incident response plan
- [ ] Check laptop battery and charger
- [ ] Know your escalation contacts

### During Your Shift

- [ ] Respond to alerts within SLA (5 min for SEV-1)
- [ ] Keep laptop nearby at all times
- [ ] Maintain sobriety (no alcohol)
- [ ] Have backup transportation if needed
- [ ] Update #on-call channel daily with status

### After Your Shift

- [ ] Hand off any ongoing incidents
- [ ] Document unresolved issues
- [ ] Update runbooks with learnings
- [ ] Report any tool/process issues

### Escalation

If you need help:
1. Post in #incident channel
2. @ mention relevant team members
3. Use PagerDuty escalation policy
4. Call DevOps lead directly if critical

**Don't hesitate to escalate!** Better safe than sorry.

---

## Tools and Resources

### Monitoring & Observability

- **Grafana:** https://api.yourdomain.com:3100 (username: admin)
- **Prometheus:** https://api.yourdomain.com:9090
- **Jaeger:** https://api.yourdomain.com:16686
- **Sentry:** https://sentry.io/organizations/...

### Communication

- **Status Page:** https://status.yourdomain.com
- **Slack:** #production-alerts, #incident-YYYYMMDD-XX
- **PagerDuty:** https://yourdomain.pagerduty.com

### Documentation

- **Rollback Runbook:** `/docs/ROLLBACK_RUNBOOK.md`
- **Deployment Checklist:** `/PRODUCTION_DEPLOYMENT_CHECKLIST.md`
- **Architecture Docs:** `/docs/ARCHITECTURE.md`
- **Runbooks:** `/docs/runbooks/`

### Scripts

- **Quick Rollback:** `/scripts/quick-rollback.sh <version>`
- **Smoke Tests:** `/scripts/smoke-tests.sh`
- **Database Restore:** `/scripts/restore.sh -d <date>`

---

## Appendix

### Incident Log

Track all production incidents in `incidents/log.md`:

| ID | Date | Severity | Duration | Root Cause | Post-Mortem |
|----|------|----------|----------|------------|-------------|
| INC-20251115-01 | 2025-11-15 | SEV-2 | 45 min | Database connection pool exhausted | [Link](#) |
| INC-20251114-01 | 2025-11-14 | SEV-3 | 2 hours | Memory leak in chat service | [Link](#) |

### Metrics to Watch

| Metric | Normal | Warning | Critical | Alert |
|--------|--------|---------|----------|-------|
| Error Rate | <0.1% | 0.5-1% | >1% | >5% |
| P95 Response Time (Auth) | <100ms | 100-200ms | >200ms | >500ms |
| P95 Response Time (Chat) | <500ms | 500-1000ms | >1000ms | >2000ms |
| CPU Usage | <40% | 40-70% | >70% | >90% |
| Memory Usage | <60% | 60-80% | >80% | >95% |
| Database Connections | <50 | 50-100 | >100 | >150 |
| Queue Depth | <100 | 100-500 | >500 | >1000 |

---

**Document Version:** 2.0
**Last Updated:** 2025-11-15
**Next Review:** Quarterly

**Maintained By:** DevOps Team
**Contact:** devops@yourdomain.com

**Remember:** Stay calm, communicate clearly, and don't be afraid to ask for help!
